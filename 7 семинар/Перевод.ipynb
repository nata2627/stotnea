{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets torch scipy scikit-learn accelerate evaluate nltk rouge_score sentencepiece sacrebleu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-23T10:05:53.209174Z","iopub.execute_input":"2025-01-23T10:05:53.209480Z","iopub.status.idle":"2025-01-23T10:05:59.982770Z","shell.execute_reply.started":"2025-01-23T10:05:53.209445Z","shell.execute_reply":"2025-01-23T10:05:59.981929Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import (\n    T5Tokenizer,\n    T5ForConditionalGeneration,\n    Trainer,\n    Seq2SeqTrainingArguments\n)\nimport torch\nfrom torch.utils.data import Dataset\nimport random\n\nclass TranslationDataset(Dataset):\n    def __init__(self, texts, translations, tokenizer):\n        self.inputs = tokenizer(\n            [\"translate English to French: \" + text for text in texts],\n            max_length=512,\n            truncation=True,\n            padding='max_length',\n            return_tensors=\"pt\"\n        )\n        self.targets = tokenizer(\n            translations,\n            max_length=512,\n            truncation=True,\n            padding='max_length',\n            return_tensors=\"pt\"\n        )\n\n    def __len__(self):\n        return len(self.targets[\"input_ids\"])\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.inputs[\"input_ids\"][idx],\n            \"attention_mask\": self.inputs[\"attention_mask\"][idx],\n            \"labels\": self.targets[\"input_ids\"][idx]\n        }\n\ndef train_translator():\n    # Load model and tokenizer\n    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").cuda()\n\n    # Load dataset\n    dataset = load_dataset(\"opus_books\", \"en-fr\", split=\"train[:10%]\", trust_remote_code=True)\n\n\n    # Take share of the loaded data if needed\n    total_examples = len(dataset)\n    subsample_size = total_examples\n\n    # Randomly sample indices\n    all_indices = list(range(total_examples))\n    selected_indices = random.sample(all_indices, subsample_size)\n\n    # Get subsampled data\n    texts = [dataset[i][\"translation\"][\"en\"] for i in selected_indices]\n    translations = [dataset[i][\"translation\"][\"fr\"] for i in selected_indices]\n\n    # Create dataset\n    train_size = int(len(texts) * 0.8)\n    train_dataset = TranslationDataset(texts[:train_size], translations[:train_size], tokenizer)\n    eval_dataset = TranslationDataset(texts[train_size:], translations[train_size:], tokenizer)\n\n    # Training configuration\n    training_args = Seq2SeqTrainingArguments(\n        output_dir=\"./results\",\n        evaluation_strategy=\"epoch\",\n        report_to=\"tensorboard\",\n        learning_rate=1e-4,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        weight_decay=0.01,\n        save_total_limit=2,\n        num_train_epochs=1,\n        predict_with_generate=True,\n        logging_dir=\"./logs\",\n        logging_steps=5,\n        push_to_hub=False,\n        save_strategy=\"epoch\"\n    )\n\n    # Training\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset\n    )\n\n    trainer.train()\n    trainer.save_model(\"./translator\")\n    return model, tokenizer\n\ndef translate_text(text, model, tokenizer):\n    inputs = tokenizer(\"translate English to French: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n    # Move inputs to CUDA\n    inputs = {k: v.cuda() for k, v in inputs.items()}\n\n    translation_ids = model.generate(\n        inputs[\"input_ids\"],\n        max_length=512,\n        min_length=10,\n        num_beams=4,\n        length_penalty=0.6,\n        early_stopping=True\n    )\n    return tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n\ndef main():\n    # Train model\n    model, tokenizer = train_translator()\n\n    # Test model\n    test_text = \"\"\"\n    The artificial intelligence has revolutionized many aspects of our daily lives,\n    bringing innovations in various fields such as medicine and education.\n    \"\"\"\n\n    translation = translate_text(test_text, model, tokenizer)\n    print(\"\\nOriginal text:\", test_text)\n    print(\"\\nFrench translation:\", translation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T10:05:59.983823Z","iopub.execute_input":"2025-01-23T10:05:59.984141Z","iopub.status.idle":"2025-01-23T10:06:22.147862Z","shell.execute_reply.started":"2025-01-23T10:05:59.984116Z","shell.execute_reply":"2025-01-23T10:06:22.147213Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T10:06:22.148573Z","iopub.execute_input":"2025-01-23T10:06:22.149052Z","iopub.status.idle":"2025-01-23T10:15:38.977825Z","shell.execute_reply.started":"2025-01-23T10:06:22.149031Z","shell.execute_reply":"2025-01-23T10:15:38.976881Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c3f6ee6d9a2472c835764809243bcf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06ccde28187f4609b89fa4666f98950a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ae0d1fe8a5e405584d4e24963a4612b"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb061fbe62794fcc9b737eac20ac71c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d118423357b498892872e85b96b4bd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff32bd362fc6441badf78bab7793876e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/28.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c39ec31032074dbb83553b987ce01277"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc79da870845481f88cf94556b963912"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/127085 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b2da2bfa38047f5a18a610a47b64229"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1271' max='1271' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1271/1271 08:55, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.189800</td>\n      <td>0.167100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nOriginal text: \n    The artificial intelligence has revolutionized many aspects of our daily lives,\n    bringing innovations in various fields such as medicine and education.\n    \n\nFrench translation: Lâ€™intelligence artificielle a rÃ©volutionnÃ© de nombreux aspects de notre vie quotidienne, apportant des innovations dans divers domaines comme la mÃ©decine et lâ€™Ã©ducation.\n","output_type":"stream"}],"execution_count":3}]}