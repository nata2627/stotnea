{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers datasets torch scipy scikit-learn accelerate evaluate nltk rouge_score sentencepiece sacrebleu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:38:06.550034Z","iopub.execute_input":"2025-01-23T07:38:06.550328Z","iopub.status.idle":"2025-01-23T07:38:36.521483Z","shell.execute_reply.started":"2025-01-23T07:38:06.550280Z","shell.execute_reply":"2025-01-23T07:38:36.520435Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import (\n    T5Tokenizer,\n    T5ForConditionalGeneration,\n    Trainer,\n    Seq2SeqTrainingArguments\n)\nimport torch\nfrom torch.utils.data import Dataset\nimport random\n\nclass QADataset(Dataset):\n    def __init__(self, contexts, questions, answers, tokenizer):\n        # Format: \"<start_question> {question} <end_question> <start_context> {context} <end_context>\"\n        self.inputs = tokenizer(\n            [f\"<start_question> {q} <end_question> <start_context> {c} <end_context>\" for q, c in zip(questions, contexts)],\n            max_length=512,\n            truncation=True,\n            padding='max_length',\n            return_tensors=\"pt\"\n        )\n        self.targets = tokenizer(\n            answers,\n            max_length=128,\n            truncation=True,\n            padding='max_length',\n            return_tensors=\"pt\"\n        )\n\n    def __len__(self):\n        return len(self.targets[\"input_ids\"])\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.inputs[\"input_ids\"][idx],\n            \"attention_mask\": self.inputs[\"attention_mask\"][idx],\n            \"labels\": self.targets[\"input_ids\"][idx]\n        }\n\ndef train_qa_model():\n    # Load model and tokenizer\n    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").cuda()\n\n    # Add special tokens for QA task\n    special_tokens_dict = {\n        'additional_special_tokens': [\n            '<start_question>', '<end_question>',\n            '<start_context>', '<end_context>'\n        ]\n    }\n    tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n\n    # Load SQuAD dataset\n    dataset = load_dataset(\"squad\", split=\"train[:10%]\", trust_remote_code=True)\n\n    # Take share of the loaded data if needed\n    total_examples = len(dataset)\n    subsample_size = total_examples\n\n    # Randomly sample indices\n    all_indices = list(range(total_examples))\n    selected_indices = random.sample(all_indices, subsample_size)\n\n    # Get subsampled data\n    contexts = [dataset[i][\"context\"] for i in selected_indices]\n    questions = [dataset[i][\"question\"] for i in selected_indices]\n    answers = [dataset[i][\"answers\"][\"text\"][0] for i in selected_indices]  # Taking first answer if multiple exist\n\n    # Create dataset\n    train_size = int(len(contexts) * 0.8)\n    train_dataset = QADataset(\n        contexts[:train_size],\n        questions[:train_size],\n        answers[:train_size],\n        tokenizer\n    )\n    eval_dataset = QADataset(\n        contexts[train_size:],\n        questions[train_size:],\n        answers[train_size:],\n        tokenizer\n    )\n\n    # Training configuration\n    training_args = Seq2SeqTrainingArguments(\n        output_dir=\"./results\",\n        evaluation_strategy=\"epoch\",\n        report_to=\"tensorboard\",\n        learning_rate=1e-4,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        weight_decay=0.01,\n        save_total_limit=2,\n        num_train_epochs=1,\n        predict_with_generate=True,\n        logging_dir=\"./logs\",\n        logging_steps=5,\n        push_to_hub=False,\n        save_strategy=\"epoch\"\n    )\n\n    # Training\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset\n    )\n\n    trainer.train()\n    trainer.save_model(\"./qa_model\")\n    return model, tokenizer\n\ndef answer_question(context, question, model, tokenizer):\n    input_text = f\"<start_question> {question} <end_question> <start_context> {context} <end_context>\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n\n    # Move inputs to CUDA\n    inputs = {k: v.cuda() for k, v in inputs.items()}\n\n    answer_ids = model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens =128,\n        min_new_tokens =1,\n        num_beams=4,\n        length_penalty=0.6,\n        early_stopping=True\n    )\n    return tokenizer.decode(answer_ids[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:59:12.173640Z","iopub.execute_input":"2025-01-23T07:59:12.173979Z","iopub.status.idle":"2025-01-23T07:59:12.185686Z","shell.execute_reply.started":"2025-01-23T07:59:12.173949Z","shell.execute_reply":"2025-01-23T07:59:12.184699Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:59:21.286761Z","iopub.execute_input":"2025-01-23T07:59:21.287160Z","iopub.status.idle":"2025-01-23T08:03:00.291691Z","shell.execute_reply.started":"2025-01-23T07:59:21.287113Z","shell.execute_reply":"2025-01-23T08:03:00.290738Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='876' max='876' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [876/876 03:23, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.017600</td>\n      <td>0.012601</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nContext: \n    The Apollo program was the third United States human spaceflight program carried out\n    by NASA. It accomplished landing the first humans on the Moon from 1969 to 1972.\n    During the Apollo 11 mission, astronauts Neil Armstrong and Buzz Aldrin landed their\n    lunar module and walked on the lunar surface, while Michael Collins remained in lunar orbit.\n    \n\nAnswering questions:\n\nQ: Who was the first person to walk on the Moon?\nA: Neil Armstrong and Buzz Aldrin\n\nQ: What was the name of the space program?\nA: Apollo\n\nQ: How many astronauts landed on the Moon during Apollo 11?\nA: Neil Armstrong\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}