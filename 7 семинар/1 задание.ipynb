{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKMq7dp2W15Y",
        "outputId": "984f3686-f910-4b07-d56e-4278db10c008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBkqaK5bawXN",
        "outputId": "c9345e1f-8394-4416-b891-db35c87336c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qOQwNlZbFiO",
        "outputId": "17123e05-c337-4d6c-d12f-2b9aa6f5b68c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/datasets\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive/datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmWCBWxrBUB3"
      },
      "source": [
        "## 1. Генерирование русских имен при помощи RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "990obDBwCC7V"
      },
      "source": [
        "Датасет: https://disk.yandex.ru/i/2yt18jHUgVEoIw\n",
        "\n",
        "1.1 На основе файла name_rus.txt создайте датасет.\n",
        "  * Учтите, что имена могут иметь различную длину\n",
        "  * Добавьте 4 специальных токена:\n",
        "    * `<PAD>` для дополнения последовательности до нужной длины;\n",
        "    * `<UNK>` для корректной обработки ранее не встречавшихся токенов;\n",
        "    * `<SOS>` для обозначения начала последовательности;\n",
        "    * `<EOS>` для обозначения конца последовательности.\n",
        "  * Преобразовывайте строку в последовательность индексов с учетом следующих замечаний:\n",
        "    * в начало последовательности добавьте токен `<SOS>`;\n",
        "    * в конец последовательности добавьте токен `<EOS>` и, при необходимости, несколько токенов `<PAD>`;\n",
        "  * `Dataset.__get_item__` возращает две последовательности: последовательность для обучения и правильный ответ.\n",
        "  \n",
        "  Пример:\n",
        "  ```\n",
        "  s = 'The cat sat on the mat'\n",
        "  # преобразуем в индексы\n",
        "  s_idx = [2, 5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
        "  # получаем x и y (__getitem__)\n",
        "  x = [2, 5, 1, 2, 8, 4, 7, 3, 0]\n",
        "  y = [5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
        "  ```\n",
        "\n",
        "1.2 Создайте и обучите модель для генерации фамилии.\n",
        "\n",
        "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`;\n",
        "  * Используйте рекуррентные слои;\n",
        "  * Задача ставится как предсказание следующего токена в каждом примере из пакета для каждого момента времени. Т.е. в данный момент времени по текущей подстроке предсказывает следующий символ для данной строки (задача классификации);\n",
        "  * Примерная схема реализации метода `forward`:\n",
        "  ```\n",
        "    input_X: [batch_size x seq_len] -> nn.Embedding -> emb_X: [batch_size x seq_len x embedding_size]\n",
        "    emb_X: [batch_size x seq_len x embedding_size] -> nn.RNN -> output: [batch_size x seq_len x hidden_size]\n",
        "    output: [batch_size x seq_len x hidden_size] -> torch.Tensor.reshape -> output: [batch_size * seq_len x hidden_size]\n",
        "    output: [batch_size * seq_len x hidden_size] -> nn.Linear -> output: [batch_size * seq_len x vocab_size]\n",
        "  ```\n",
        "\n",
        "1.3 Напишите функцию, которая генерирует фамилию при помощи обученной модели:\n",
        "  * Построение начинается с последовательности единичной длины, состоящей из индекса токена `<SOS>`;\n",
        "  * Начальное скрытое состояние RNN `h_t = None`;\n",
        "  * В результате прогона последнего токена из построенной последовательности через модель получаете новое скрытое состояние `h_t` и распределение над всеми токенами из словаря;\n",
        "  * Выбираете 1 токен пропорционально вероятности и добавляете его в последовательность (можно воспользоваться `torch.multinomial`);\n",
        "  * Повторяете эти действия до тех пор, пока не сгенерирован токен `<EOS>` или не превышена максимальная длина последовательности.\n",
        "\n",
        "При обучении каждые `k` эпох генерируйте несколько фамилий и выводите их на экран."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "# Чтение данных из файла\n",
        "with open('name_rus.txt', 'r', encoding='windows-1251') as f:\n",
        "    names = f.read().splitlines()\n",
        "\n",
        "# Создание словаря токенов\n",
        "special_tokens = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n",
        "all_chars = sorted(set(''.join(names))) + special_tokens\n",
        "char2idx = {char: idx for idx, char in enumerate(all_chars)}\n",
        "idx2char = {idx: char for char, idx in char2idx.items()}\n",
        "\n",
        "# Преобразование имен в индексы\n",
        "def encode_name(name, max_len=20):\n",
        "    encoded = [char2idx['<SOS>']] + [char2idx.get(char, char2idx['<UNK>']) for char in name]\n",
        "    encoded += [char2idx['<EOS>']]\n",
        "    encoded += [char2idx['<PAD>']] * (max_len - len(encoded))\n",
        "    return encoded[:max_len]\n",
        "\n",
        "# Декодирование индексов обратно в имя\n",
        "def decode_name(indices):\n",
        "    return ''.join([idx2char[idx] for idx in indices if idx2char[idx] not in special_tokens])\n",
        "\n",
        "# Создание датасета\n",
        "class NameDataset(Dataset):\n",
        "    def __init__(self, names, max_len=20):\n",
        "        self.data = [encode_name(name, max_len) for name in names]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx][:-1]\n",
        "        y = self.data[idx][1:]\n",
        "        return torch.tensor(x), torch.tensor(y)\n",
        "\n",
        "dataset = NameDataset(names)"
      ],
      "metadata": {
        "id": "veXjAEieA9al"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Модель LSTM\n",
        "class NameGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2):\n",
        "        super(NameGenerator, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = out.reshape(-1, out.size(2))\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_dim))\n",
        "\n",
        "# Функция для генерации фамилий\n",
        "def generate_name(model, max_len=20):\n",
        "    model.eval()\n",
        "    input_seq = torch.tensor([[char2idx['<SOS>']]])\n",
        "    hidden = None\n",
        "    generated_name = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "        probs = torch.softmax(output, dim=-1)\n",
        "        next_char_idx = torch.multinomial(probs[-1], num_samples=1)\n",
        "        next_char = next_char_idx.item()\n",
        "\n",
        "        if idx2char[next_char] == '<EOS>':\n",
        "            break\n",
        "        generated_name.append(idx2char[next_char])\n",
        "        input_seq = next_char_idx.unsqueeze(0)\n",
        "\n",
        "    return ''.join(generated_name)\n",
        "\n",
        "# Гиперпараметры\n",
        "embedding_dim = 32\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "vocab_size = len(char2idx)\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "max_len = 20\n",
        "\n",
        "# Подготовка данных\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Инициализация модели\n",
        "model = NameGenerator(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=char2idx['<PAD>'])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Генерация фамилий каждые 5 эпох\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for x, y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        hidden = model.init_hidden(x.size(0))\n",
        "        output, hidden = model(x, hidden)\n",
        "        loss = criterion(output, y.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(dataloader):.4f}\")\n",
        "\n",
        "    # Генерация фамилий каждые 5 эпох\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(\"Generated names:\")\n",
        "        for _ in range(5):\n",
        "            print(generate_name(model))\n",
        "\n",
        "# Пример использования\n",
        "print(\"Generated name:\", generate_name(model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdHmoZhDBGsQ",
        "outputId": "47d89120-23f9-4143-f095-305ef45ab282"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 3.1103\n",
            "Epoch 2, Loss: 2.6773\n",
            "Epoch 3, Loss: 2.4485\n",
            "Epoch 4, Loss: 2.2828\n",
            "Epoch 5, Loss: 2.1603\n",
            "Generated names:\n",
            "гккадя\n",
            "донетка\n",
            "локюонка\n",
            "лиуью\n",
            "гиляшиса\n",
            "Epoch 6, Loss: 2.0950\n",
            "Epoch 7, Loss: 2.0445\n",
            "Epoch 8, Loss: 1.9792\n",
            "Epoch 9, Loss: 1.9196\n",
            "Epoch 10, Loss: 1.8825\n",
            "Generated names:\n",
            "тируша\n",
            "фетюня\n",
            "егуня\n",
            "пэстьонка\n",
            "малюха\n",
            "Epoch 11, Loss: 1.8437\n",
            "Epoch 12, Loss: 1.8192\n",
            "Epoch 13, Loss: 1.7684\n",
            "Epoch 14, Loss: 1.7353\n",
            "Epoch 15, Loss: 1.7187\n",
            "Generated names:\n",
            "лада\n",
            "мольша\n",
            "моля\n",
            "даня\n",
            "амоныха\n",
            "Epoch 16, Loss: 1.6754\n",
            "Epoch 17, Loss: 1.6433\n",
            "Epoch 18, Loss: 1.6247\n",
            "Epoch 19, Loss: 1.5964\n",
            "Epoch 20, Loss: 1.5684\n",
            "Generated names:\n",
            "денена\n",
            "спатя\n",
            "юлиан\n",
            "ледий\n",
            "боря\n",
            "Epoch 21, Loss: 1.5445\n",
            "Epoch 22, Loss: 1.5260\n",
            "Epoch 23, Loss: 1.5149\n",
            "Epoch 24, Loss: 1.4938\n",
            "Epoch 25, Loss: 1.4787\n",
            "Generated names:\n",
            "кизюша\n",
            "геруша\n",
            "сева\n",
            "оман\n",
            "фешуся\n",
            "Epoch 26, Loss: 1.4550\n",
            "Epoch 27, Loss: 1.4433\n",
            "Epoch 28, Loss: 1.4297\n",
            "Epoch 29, Loss: 1.4112\n",
            "Epoch 30, Loss: 1.4048\n",
            "Generated names:\n",
            "максюша\n",
            "няха\n",
            "брина\n",
            "анокулка\n",
            "торюха\n",
            "Epoch 31, Loss: 1.3827\n",
            "Epoch 32, Loss: 1.3667\n",
            "Epoch 33, Loss: 1.3543\n",
            "Epoch 34, Loss: 1.3485\n",
            "Epoch 35, Loss: 1.3399\n",
            "Generated names:\n",
            "петяра\n",
            "декабрита\n",
            "натаха\n",
            "сюра\n",
            "велор\n",
            "Epoch 36, Loss: 1.3335\n",
            "Epoch 37, Loss: 1.3223\n",
            "Epoch 38, Loss: 1.3095\n",
            "Epoch 39, Loss: 1.3119\n",
            "Epoch 40, Loss: 1.2982\n",
            "Generated names:\n",
            "людамий\n",
            "ларюха\n",
            "нароша\n",
            "валентина\n",
            "мариля\n",
            "Epoch 41, Loss: 1.2879\n",
            "Epoch 42, Loss: 1.2845\n",
            "Epoch 43, Loss: 1.2763\n",
            "Epoch 44, Loss: 1.2683\n",
            "Epoch 45, Loss: 1.2607\n",
            "Generated names:\n",
            "илианикыч\n",
            "адя\n",
            "жрона\n",
            "никиха\n",
            "душа\n",
            "Epoch 46, Loss: 1.2505\n",
            "Epoch 47, Loss: 1.2465\n",
            "Epoch 48, Loss: 1.2464\n",
            "Epoch 49, Loss: 1.2508\n",
            "Epoch 50, Loss: 1.2393\n",
            "Generated names:\n",
            "агрипинка\n",
            "вероник\n",
            "витяшка\n",
            "тимофияна\n",
            "петлянй\n",
            "Epoch 51, Loss: 1.2322\n",
            "Epoch 52, Loss: 1.2204\n",
            "Epoch 53, Loss: 1.2256\n",
            "Epoch 54, Loss: 1.2135\n",
            "Epoch 55, Loss: 1.2065\n",
            "Generated names:\n",
            "авдокитьич\n",
            "олена\n",
            "галька\n",
            "анастюха\n",
            "мита\n",
            "Epoch 56, Loss: 1.2163\n",
            "Epoch 57, Loss: 1.2108\n",
            "Epoch 58, Loss: 1.2037\n",
            "Epoch 59, Loss: 1.1979\n",
            "Epoch 60, Loss: 1.1911\n",
            "Generated names:\n",
            "федорка\n",
            "наша\n",
            "лавруша\n",
            "алюля\n",
            "адема\n",
            "Epoch 61, Loss: 1.2010\n",
            "Epoch 62, Loss: 1.1950\n",
            "Epoch 63, Loss: 1.1882\n",
            "Epoch 64, Loss: 1.1845\n",
            "Epoch 65, Loss: 1.1920\n",
            "Generated names:\n",
            "нинуля\n",
            "лодуся\n",
            "эфросинья\n",
            "ангелинка\n",
            "артемыч\n",
            "Epoch 66, Loss: 1.1790\n",
            "Epoch 67, Loss: 1.1812\n",
            "Epoch 68, Loss: 1.1822\n",
            "Epoch 69, Loss: 1.1896\n",
            "Epoch 70, Loss: 1.1673\n",
            "Generated names:\n",
            "арсуша\n",
            "александр\n",
            "туля\n",
            "лидоня\n",
            "юргина\n",
            "Epoch 71, Loss: 1.1720\n",
            "Epoch 72, Loss: 1.1718\n",
            "Epoch 73, Loss: 1.1716\n",
            "Epoch 74, Loss: 1.1731\n",
            "Epoch 75, Loss: 1.1658\n",
            "Generated names:\n",
            "ефросиния\n",
            "мюташа\n",
            "мирон\n",
            "владимирыч\n",
            "павелка\n",
            "Epoch 76, Loss: 1.1657\n",
            "Epoch 77, Loss: 1.1598\n",
            "Epoch 78, Loss: 1.1663\n",
            "Epoch 79, Loss: 1.1607\n",
            "Epoch 80, Loss: 1.1586\n",
            "Generated names:\n",
            "ганя\n",
            "заня\n",
            "игоряша\n",
            "кулина\n",
            "кирюня\n",
            "Epoch 81, Loss: 1.1520\n",
            "Epoch 82, Loss: 1.1541\n",
            "Epoch 83, Loss: 1.1502\n",
            "Epoch 84, Loss: 1.1585\n",
            "Epoch 85, Loss: 1.1528\n",
            "Generated names:\n",
            "лукич\n",
            "машоня\n",
            "семенка\n",
            "лаврентьюшка\n",
            "галлка\n",
            "Epoch 86, Loss: 1.1526\n",
            "Epoch 87, Loss: 1.1533\n",
            "Epoch 88, Loss: 1.1462\n",
            "Epoch 89, Loss: 1.1496\n",
            "Epoch 90, Loss: 1.1519\n",
            "Generated names:\n",
            "степанида\n",
            "емилия\n",
            "манюша\n",
            "николич\n",
            "леонидка\n",
            "Epoch 91, Loss: 1.1457\n",
            "Epoch 92, Loss: 1.1460\n",
            "Epoch 93, Loss: 1.1466\n",
            "Epoch 94, Loss: 1.1399\n",
            "Epoch 95, Loss: 1.1455\n",
            "Generated names:\n",
            "винюша\n",
            "митраша\n",
            "владя\n",
            "степанида\n",
            "риммыч\n",
            "Epoch 96, Loss: 1.1381\n",
            "Epoch 97, Loss: 1.1305\n",
            "Epoch 98, Loss: 1.1405\n",
            "Epoch 99, Loss: 1.1326\n",
            "Epoch 100, Loss: 1.1374\n",
            "Generated names:\n",
            "данил\n",
            "федуня\n",
            "германик\n",
            "аксюша\n",
            "ема\n",
            "Generated name: христя\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJf5iaA2fOTM"
      },
      "source": [
        "## 2. Генерирование текста при помощи RNN\n",
        "\n",
        "2.1 Скачайте из интернета какое-нибудь художественное произведение\n",
        "  * Выбирайте достаточно крупное произведение, чтобы модель лучше обучалась;\n",
        "\n",
        "2.2 На основе выбранного произведения создайте датасет.\n",
        "\n",
        "Отличия от задачи 1:\n",
        "  * Токены <SOS>, `<EOS>` и `<UNK>` можно не добавлять;\n",
        "  * При создании датасета текст необходимо предварительно разбить на части. Выберите желаемую длину последовательности `seq_len` и разбейте текст на построки длины `seq_len` (можно без перекрытия, можно с небольшим перекрытием).\n",
        "\n",
        "2.3 Создайте и обучите модель для генерации текста\n",
        "  * Задача ставится точно так же как в 1.2;\n",
        "  * При необходимости можете применить:\n",
        "    * двухуровневые рекуррентные слои (`num_layers`=2)\n",
        "    * [обрезку градиентов](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
        "\n",
        "2.4 Напишите функцию, которая генерирует фрагмент текста при помощи обученной модели\n",
        "  * Процесс генерации начинается с небольшого фрагмента текста `prime`, выбранного вами (1-2 слова)\n",
        "  * Сначала вы пропускаете через модель токены из `prime` и генерируете на их основе скрытое состояние рекуррентного слоя `h_t`;\n",
        "  * После этого вы генерируете строку нужной длины аналогично 1.3\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Загрузка текста из файла\n",
        "with open(\"dostoevsky_cut.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read().lower()\n",
        "\n",
        "# Создание словаря токенов\n",
        "chars = sorted(set(text))\n",
        "char2idx = {char: idx for idx, char in enumerate(chars)}\n",
        "idx2char = {idx: char for idx, char in enumerate(chars)}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Преобразование текста в последовательность индексов\n",
        "def text_to_indices(text):\n",
        "    return [char2idx[char] for char in text]\n",
        "\n",
        "# Разделение текста на последовательности\n",
        "def create_sequences(data, seq_len):\n",
        "    sequences = []\n",
        "    for i in range(0, len(data) - seq_len, seq_len):\n",
        "        sequences.append(data[i:i + seq_len + 1])  # +1 для правильного ответа\n",
        "    return sequences\n",
        "\n",
        "# Создание датасета\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, sequences):\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = self.sequences[idx]\n",
        "        x = torch.tensor(sequence[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(sequence[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "# Гиперпараметры\n",
        "seq_len = 100  # Длина последовательности\n",
        "batch_size = 64\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "num_layers = 2\n",
        "epochs = 20\n",
        "learning_rate = 0.001\n",
        "clip_value = 5  # Обрезка градиентов\n",
        "\n",
        "# Подготовка данных\n",
        "data = text_to_indices(text)\n",
        "sequences = create_sequences(data, seq_len)\n",
        "dataset = TextDataset(sequences)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Модель RNN\n",
        "class TextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
        "        super(TextGenerator, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        out = out.reshape(-1, out.size(2))\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_dim))\n",
        "\n",
        "# Инициализация модели\n",
        "model = TextGenerator(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Обучение модели\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for x, y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        hidden = model.init_hidden(x.size(0))\n",
        "        output, hidden = model(x, hidden)\n",
        "        loss = criterion(output, y.view(-1))\n",
        "        loss.backward()\n",
        "        # Обрезка градиентов\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(dataloader):.4f}\")\n",
        "\n",
        "# Функция для генерации текста\n",
        "def generate_text(model, prime, length=200, temperature=0.8):\n",
        "    model.eval()\n",
        "    hidden = None\n",
        "    prime_indices = text_to_indices(prime)\n",
        "    input_seq = torch.tensor([prime_indices], dtype=torch.long)\n",
        "\n",
        "    # Пропуск prime через модель для получения скрытого состояния\n",
        "    for char in prime_indices[:-1]:\n",
        "        _, hidden = model(input_seq, hidden)\n",
        "        input_seq = torch.tensor([[char]], dtype=torch.long)\n",
        "\n",
        "    generated_text = prime\n",
        "    input_seq = torch.tensor([[prime_indices[-1]]], dtype=torch.long)\n",
        "\n",
        "    # Генерация текста\n",
        "    for _ in range(length):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "        probs = torch.softmax(output / temperature, dim=-1).squeeze()\n",
        "        next_char_idx = torch.multinomial(probs, num_samples=1).item()\n",
        "        next_char = idx2char[next_char_idx]\n",
        "        generated_text += next_char\n",
        "        input_seq = torch.tensor([[next_char_idx]], dtype=torch.long)\n",
        "    return generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDEoeJprBBJT",
        "outputId": "e95bab75-4a0c-4274-bd28-9463802290ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 3.0202\n",
            "Epoch 2, Loss: 2.4097\n",
            "Epoch 3, Loss: 2.1767\n",
            "Epoch 4, Loss: 2.0312\n",
            "Epoch 5, Loss: 1.9299\n",
            "Epoch 6, Loss: 1.8534\n",
            "Epoch 7, Loss: 1.7946\n",
            "Epoch 8, Loss: 1.7463\n",
            "Epoch 9, Loss: 1.7067\n",
            "Epoch 10, Loss: 1.6725\n",
            "Epoch 11, Loss: 1.6439\n",
            "Epoch 12, Loss: 1.6184\n",
            "Epoch 13, Loss: 1.5966\n",
            "Epoch 14, Loss: 1.5753\n",
            "Epoch 15, Loss: 1.5579\n",
            "Epoch 16, Loss: 1.5424\n",
            "Epoch 17, Loss: 1.5278\n",
            "Epoch 18, Loss: 1.5119\n",
            "Epoch 19, Loss: 1.4986\n",
            "Epoch 20, Loss: 1.4858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример генерации текста\n",
        "prime_text = \"скучно\"\n",
        "generated_text = generate_text(model, prime_text, length=300)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRsQ9ccAOijm",
        "outputId": "1f4fbafa-6810-48ee-80a4-8361e74bfdbd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "скучности, — я неужели и начиналось виде подроднулся ко годовой и вергновными николаевны и тогда так были по-настого взглядо-собидно, что ему при чего-нибудь заключить к нам любопытских словом, что позволите, как теперь ангельчика. начали бы только что вас скажу, только после как к вам девушки какий-то сч\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}